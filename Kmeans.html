<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, minimal-ui">
    <title>Choose the right ML tool and use it like a pro!</title>
    <link type="text/css" rel="stylesheet" href="assets/css/github-markdown.css">
    <link type="text/css" rel="stylesheet" href="assets/css/pilcrow.css">
    <link type="text/css" rel="stylesheet" href="assets/css/hljs-github.min.css"/>
  </head>
  <body>
    <article class="markdown-body"><h1 id="choose-the-right-ml-tool-and-use-it-like-a-pro!"><a class="header-link" href="#choose-the-right-ml-tool-and-use-it-like-a-pro!"></a>Choose the right ML tool and use it like a pro!</h1>
<p>K-means is a widely used clustering algorithm in machine learning community. 
It iteratively computes the distance between each training point to every centroid, 
re-assigns the training point to new cluster and re-compute the new centroid of each cluster. </p>
<p>This hands-on includes two tasks</p>
<ul class="list">
<li>Write a Harp-DAAL K-means program by using Java API (8 steps)</li>
<li>Run and tune Harp-DAAL K-means from an image clustering application via python API (5 steps)</li>
</ul>
<p>Users are supposed to get an access to a machine with sudo permission and pre-installed docker environment. </p>
<h2 id="prerequisites"><a class="header-link" href="#prerequisites"></a>Prerequisites</h2>
<h3 id="download-docker-image-and-launch-container-instance"><a class="header-link" href="#download-docker-image-and-launch-container-instance"></a>Download Docker Image and launch Container Instance</h3>
<p>Download the image and launch the container. Before downloading check weather the VM already has the docker image.</p>
<pre class="hljs"><code><span class="hljs-comment"># start the docker</span>
sudo systemctl start docker
<span class="hljs-comment"># Check installed images</span>
sudo docker image ls
<span class="hljs-comment"># Download an image if not installed otherwise skip this step</span>
sudo docker pull dscspidal/harp-daal
<span class="hljs-comment"># Start a container</span>
sudo docker run -it dscspidal/harp-daal /etc/bootstrap.sh -bash</code></pre><p>After executing the last command you will be logged on to the docker image and use the 
following commands to build the latest version of harp-daal</p>
<pre class="hljs"><code><span class="hljs-built_in">cd</span> /harp
git pull origin master
mvn package

<span class="hljs-comment"># Copy all dynamic links to HDFS</span>
hdfs dfs -mkdir -p /Hadoop/Libraries &amp;&amp; \
hdfs dfs -put <span class="hljs-_">-f</span> <span class="hljs-variable">${DAALROOT}</span>/lib/intel64_lin/libJavaAPI.so /Hadoop/Libraries/ &amp;&amp; \
hdfs dfs -put <span class="hljs-_">-f</span> <span class="hljs-variable">${DAALROOT}</span>/../tbb/lib/intel64_lin/gcc4.4/libtbb* /Hadoop/Libraries/ &amp;&amp; \
hdfs dfs -put <span class="hljs-_">-f</span> <span class="hljs-variable">${HARP_DAAL_ROOT}</span>/external/omp/libiomp5.so /Hadoop/Libraries/</code></pre><p>The container takes up to 20GB disk space. If the machine has more than 50GB disk space, there shall be no problem to 
launch the container instance. Otherwise, users could use the following commands to clean up the docker space </p>
<pre class="hljs"><code><span class="hljs-comment"># Remove useless docker images</span>
sudo docker image rm &lt;useless-docker-image&gt;
<span class="hljs-comment"># Remove exited containers</span>
sudo docker rm $(sudo docker ps <span class="hljs-_">-a</span> <span class="hljs-_">-f</span> status=exited -q)
<span class="hljs-comment"># Clean up all dangling cache</span>
sudo docker system prune</code></pre><p>Find the docker container ID</p>
<pre class="hljs"><code>sudo docker ps</code></pre><p>and log into the docker </p>
<pre class="hljs"><code>sudo docker <span class="hljs-built_in">exec</span> -it &lt;container_id&gt; bash</code></pre><h3 id="dependencies-and-environment-variables"><a class="header-link" href="#dependencies-and-environment-variables"></a>Dependencies and Environment Variables</h3>
<p>The hands-on codes have the dependencies as follows,</p>
<ul class="list">
<li>Python 2.7+</li>
<li>Python module Numpy </li>
<li>Hadoop 2.6.0/Hadoop 2.6.5</li>
<li>DAAL 2018+ </li>
</ul>
<p>The following section describes where the important components of the Tutorial are</p>
<ol class="list">
<li>Harp Source Code - /harp                                                                   </li>
<li>Hadoop Installation - /usr/local/hadoop                                                        </li>
<li>K-Means tutorial code - /harp/harp-daal-app/src/main/java/edu/iu/daal_tutorial/daal_kmeans      </li>
<li>Python Code - /harp/harp-daal-python/examples/daal/                                   </li>
</ol>
<p>The docker image already includes them and other tools, the image has the following machine learning algorithms</p>
<pre class="hljs"><code><span class="hljs-comment"># List of HarpDaal applications (in harp-daal-&lt;version&gt;.jar)</span>
edu.iu.daal_als.ALSDaalLauncher
edu.iu.daal_cov.COVDaalLauncher
edu.iu.daal_kmeans.regroupallgather.KMeansDaalLauncher
edu.iu.daal_linreg.L<span class="hljs-keyword">in</span>RegDaalLauncher
edu.iu.daal_mom.MOMDaalLauncher
edu.iu.daal_naive.NaiveDaalLauncher
edu.iu.daal_nn.NNDaalLauncher
edu.iu.daal_pca.PCADaalLauncher
edu.iu.daal_qr.QRDaalLauncher
edu.iu.daal_ridgereg.RidgeRegDaalLauncher
edu.iu.daal_sgd.SGDDaalLauncher
edu.iu.daal_svd.SVDDaalLauncher

<span class="hljs-comment"># List of HarpDaal examples in Python (/harp/harp-daal-python/examples/daal/)</span>
run_harp_daal_ALSDaal.py
run_harp_daal_COVDaal.py
run_harp_daal_KMeansDaal.py
run_harp_daal_L<span class="hljs-keyword">in</span>RegDaal.py
run_harp_daal_MOMDaal.py
run_harp_daal_NNDaal.py
run_harp_daal_NaiveDaal.py
run_harp_daal_PCADaal.py
run_harp_daal_QRDaal.py
run_harp_daal_RidgeRegDaal.py
run_harp_daal_SGDDaal.py
run_harp_daal_SVDDaal.py

<span class="hljs-comment"># Image clustering example</span>
/harp/harp-daal-python/examples/scdemo/tutorial/</code></pre><p>The <em>bootstrap</em> script shall launch a Hadoop Cluster and set up all the environment variables. 
To verify the Hadoop status</p>
<pre class="hljs"><code><span class="hljs-comment">## To check the status of HDFS</span>
<span class="hljs-variable">${HADOOP_HOME}</span>/bin/hdfs dfsadmin -report
<span class="hljs-comment">## To check the status of Yarn </span>
<span class="hljs-variable">${HADOOP_HOME}</span>/bin/yarn node -list</code></pre><p>Check out the environment variables to figure out the locations of the important files.</p>
<pre class="hljs"><code><span class="hljs-built_in">echo</span> <span class="hljs-variable">$HADOOP_HOME</span>
/usr/<span class="hljs-built_in">local</span>/hadoop

<span class="hljs-built_in">echo</span> <span class="hljs-variable">$HARP_JAR</span>
/usr/<span class="hljs-built_in">local</span>/hadoop/harp-app-1.0-SNAPSHOT.jar

<span class="hljs-built_in">echo</span> <span class="hljs-variable">$HARP_DAAL_JAR</span>
/usr/<span class="hljs-built_in">local</span>/hadoop/harp-daal-app-1.0-SNAPSHOT.jar

<span class="hljs-built_in">echo</span> <span class="hljs-variable">$DAALROOT</span>
/harp/harp-daal-app/__release__lnx/daal

<span class="hljs-built_in">echo</span> <span class="hljs-variable">$PYTHONPATH</span>
/harp/harp-daal-python</code></pre><p>If the script fails to complete these steps, users could manually set them </p>
<pre class="hljs"><code><span class="hljs-built_in">export</span> HADOOP_HOME=&lt;path to hadoop home folder&gt;
<span class="hljs-built_in">export</span> HARP_JAR=&lt;path to&gt;/harp-app-1.0-SNAPSHOT.jar
<span class="hljs-built_in">export</span> HARP_DAAL_JAR=&lt;path to&gt;/harp-daal-app-1.0-SNAPSHOT.jar
<span class="hljs-built_in">export</span> DAALROOT=&lt;path to your compiled daal folder&gt;
<span class="hljs-built_in">export</span> PYTHONPATH=&lt;path to&gt;/harp-daal-python</code></pre><p>In a situation where you would like to stop and restart Hadoop, use the following commands</p>
<pre class="hljs"><code><span class="hljs-comment">## stop all services</span>
<span class="hljs-variable">${HADOOP_HOME}</span>/sbin/stop-all.sh
<span class="hljs-comment">## launch HDFS service</span>
<span class="hljs-variable">${HADOOP_HOME}</span>/sbin/start-dfs.sh
<span class="hljs-comment">## launch yarn daemons</span>
<span class="hljs-variable">${HADOOP_HOME}</span>/sbin/start-yarn.sh</code></pre><h2 id="program-k-means-via-java-apis"><a class="header-link" href="#program-k-means-via-java-apis"></a>Program K-means via Java APIs</h2>
<p>Harp-DAAL framework provides developers of Java API to implement inter-mapper communication patterns and invoke intra-node DAAL kernels. The K-means source files are located at </p>
<pre class="hljs"><code>/harp/harp-daal-app/src/main/java/edu/iu/daal_tutorial/daal_kmeans/</code></pre><p>Lets open the source file where K-Means is implemented.</p>
<pre class="hljs"><code>vi /harp/harp-daal-app/src/main/java/edu/iu/daal_tutorial/daal_kmeans/KMeansDaalCollectiveMapper.java</code></pre><p>Users shall edit the function <em>runKmeans</em> from source file <em>KMeansDaalCollectiveMapper.java</em>. 
Currently, the function <em>runKmeans</em> is only a skeleton, and users will go through 8 steps to finish 
a complete Harp-DAAL-Kmeans application as indicated in the following code snippet.</p>
<pre class="hljs"><code><span class="hljs-function"><span class="hljs-keyword">private</span> <span class="hljs-keyword">void</span> <span class="hljs-title">runKmeans</span><span class="hljs-params">(List&lt;String&gt; fileNames, Configuration conf, Context context)</span> </span>{
  <span class="hljs-keyword">long</span> start_execution = System.currentTimeMillis();
  <span class="hljs-keyword">this</span>.fileNames = fileNames;
  <span class="hljs-keyword">this</span>.conf = conf;

  <span class="hljs-comment">//************* Step 1: load training data *********************</span>
  <span class="hljs-comment">//************* Step 2: load centroids (model) data *********************</span>
  <span class="hljs-comment">//************* Step 3: convert training data from harp to daal *********************</span>
  <span class="hljs-comment">//************* Step 4: Setup DAAL K-means kernel and cenTable at DAAL side *********************</span>
  <span class="hljs-comment">// start the iteration</span>
  <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; numIterations; i++) {
    <span class="hljs-comment">//************* Step 5: Convert Centroids data from Harp to DAAL *********************</span>
    <span class="hljs-comment">//************* Step 6: Local computation by DAAL to get partial result *********************</span>
    <span class="hljs-comment">//************* Step 7: Inter-Mapper communication *********************</span>
  }
  <span class="hljs-comment">//************* Step 8: Release Memory and Store Centroids *********************</span>
}</code></pre><p>The codes of missing steps are already packaged into private member function of <em>KMeansDaalCollectiveMapper.java</em> named <em>runKmeans_Answer</em> and you can use it to get it to working quickly. Please refer to member function definition for 
implementation details of each step. After adding codes at each step, re-compile the harp-daal-application 
by maven at the root harp directory (where the pom.xml resides) </p>
<pre class="hljs"><code><span class="hljs-built_in">cd</span> /harp
mvn package</code></pre><p>and re-run the application on Hadoop cluster</p>
<pre class="hljs"><code><span class="hljs-built_in">cd</span> /harp/harp-daal-app/test_scripts
./harp-daal-tutorial-kmeans.sh</code></pre><p>While application is running you can use Hadoop commands to manage the application.</p>
<pre class="hljs"><code><span class="hljs-built_in">cd</span> <span class="hljs-variable">$HADOOP_HOME</span>
<span class="hljs-comment">## list applications</span>
./bin/yarn application -list
<span class="hljs-comment">## kill application</span>
./bin/yarn application -kill &lt;application id&gt;</code></pre><h3 id="solution-description"><a class="header-link" href="#solution-description"></a>Solution Description</h3>
<p>The following sections describe each step of the algorithm that is left blank. You can copy the code in each step to complete the solution.</p>
<h3 id="step-1:-load-training-data-(feature-vectors)"><a class="header-link" href="#step-1:-load-training-data-(feature-vectors)"></a>Step 1: Load training data (feature vectors)</h3>
<p>Use the following function to load training data from HDFS. </p>
<pre class="hljs"><code><span class="hljs-comment">// create a pointArray</span>
List&lt;<span class="hljs-keyword">double</span>[]&gt; pointArrays = LoadTrainingData();</code></pre><h3 id="step-2:-load-model-data-(centroids)"><a class="header-link" href="#step-2:-load-model-data-(centroids)"></a>Step 2: Load model data (centroids)</h3>
<p>Similarly, create a harp table object <em>cenTable</em> and load centroids from HDFS. 
Because centroids are requested by all the mappers, load them at master mapper and 
broadcast them to all the other mappers. </p>
<pre class="hljs"><code><span class="hljs-comment">// create a table to hold centroids data</span>
Table&lt;DoubleArray&gt; cenTable = <span class="hljs-keyword">new</span> Table&lt;&gt;(<span class="hljs-number">0</span>, <span class="hljs-keyword">new</span> DoubleArrPlus());

<span class="hljs-keyword">if</span> (<span class="hljs-keyword">this</span>.isMaster()) {
  createCenTable(cenTable);
  loadCentroids(cenTable);
}
<span class="hljs-comment">// Bcast centroids to other mappers</span>
bcastCentroids(cenTable, <span class="hljs-keyword">this</span>.getMasterID());</code></pre><h3 id="step-3:-convert-training-data-from-harp-side-to-daal-side"><a class="header-link" href="#step-3:-convert-training-data-from-harp-side-to-daal-side"></a>Step 3: Convert training data from Harp side to DAAL side</h3>
<p>The training data loaded from HDFS are stored at Java heap memory. To invoke DAAL kernel, convert them to 
DAAL <em>NumericTable</em> </p>
<pre class="hljs"><code><span class="hljs-comment">// convert training data fro harp to daal</span>
NumericTable trainingdata_daal = convertTrainData(pointArrays);</code></pre><p>It allocates native memory for <em>NumericTable</em> and copy data from <em>pointArrays</em> to <em>trainingdata_daal</em></p>
<h3 id="step-4:-create-and-set-up-daal-k-means-kernel"><a class="header-link" href="#step-4:-create-and-set-up-daal-k-means-kernel"></a>Step 4: Create and set up DAAL K-means kernel</h3>
<p>DAAL provides the following Java API to invoke their low-level native kernels written for K-means</p>
<pre class="hljs"><code><span class="hljs-keyword">import</span> com.intel.daal.algorithms.kmeans.*;
<span class="hljs-keyword">import</span> com.intel.daal.algorithms.kmeans.init.*;
<span class="hljs-keyword">import</span> com.intel.daal.services.Environment;</code></pre><p>Call them by specifying the input training data object and number of centroids </p>
<pre class="hljs"><code><span class="hljs-comment">// create a daal kmeans kernel object</span>
DistributedStep1Local kmeansLocal = <span class="hljs-keyword">new</span> DistributedStep1Local(daal_Context, Double.class, Method.defaultDense, <span class="hljs-keyword">this</span>.numCentroids);
<span class="hljs-comment">// set up input training data</span>
kmeansLocal.input.set(InputId.data, trainingdata_daal);</code></pre><p>As DAAL uses MKL and TBB within its implementation, specify the number of threads used by DAAL (by default a maximum available threads on a processor)</p>
<pre class="hljs"><code><span class="hljs-comment">// specify the threads used in DAAL kernel</span>
Environment.setNumberOfThreads(numThreads);</code></pre><p>Finally, create another <em>NumericTable</em> to store centroids at DAAL side</p>
<pre class="hljs"><code><span class="hljs-comment">// create cenTable at daal side</span>
NumericTable cenTable_daal = createCenTableDAAL();</code></pre><h3 id="step-5:-convert-centroids-from-harp-to-daal"><a class="header-link" href="#step-5:-convert-centroids-from-harp-to-daal"></a>Step 5: Convert centroids from Harp to DAAL</h3>
<p>Centroids are stored in harp table <em>cenTable</em> for inter-mapper communication. Convert them 
to DAAL within each iteration of local computation. </p>
<pre class="hljs"><code><span class="hljs-comment">//Convert Centroids data from Harp to DAAL</span>
convertCenTableHarpToDAAL(cenTable, cenTable_daal);</code></pre><h3 id="step-6:-local-computation-by-daal-kernel"><a class="header-link" href="#step-6:-local-computation-by-daal-kernel"></a>Step 6: Local computation by DAAL kernel</h3>
<p>Call DAAL K-means kernels of local computation at each iteration.</p>
<pre class="hljs"><code><span class="hljs-comment">// specify centroids data to daal kernel </span>
kmeansLocal.input.set(InputId.inputCentroids, cenTable_daal);
<span class="hljs-comment">// first step of local computation by using DAAL kernels to get partial result</span>
PartialResult pres = kmeansLocal.compute();</code></pre><h3 id="step-7:-inter-mapper-communication"><a class="header-link" href="#step-7:-inter-mapper-communication"></a>Step 7: Inter-mapper communication</h3>
<p>Harp-DAAL-Kmeans adopts an <em>allreduce</em> computation model, where each mapper keeps a local copy of the whole model data (centroids). 
However, it provides different communication operations to synchronize model data among mappers. </p>
<ul class="list">
<li>Regroup &amp; Allgather (default)</li>
<li>Allreduce</li>
<li>Broadcast &amp; Reduce</li>
<li>Push-Pull</li>
</ul>
<p>All of the operations will take in two arguments, 1) <em>cenTable</em> at harp side, 2) partial results obtained from DAAL; Internally, the data is retrieved from DAAL partial results and communicated by Harp.</p>
<p>In Regroup &amp; Allgather operation, it first combines the same centroid from different mappers and re-distributes them 
to mappers by a specified order. After averaging the centroids, an allgather operation makes every mapper get 
a complete copy of the averaged centroids. </p>
<pre class="hljs"><code>comm_regroup_allgather(cenTable, pres);</code></pre><p>In Allreduce operation, the centroids are reduced and copied to every mapper. Then an average operation applies to them on each mapper. </p>
<pre class="hljs"><code>comm_allreduce(cenTable, pres);</code></pre><p>In Broadcast &amp; Reduce, it first reduces centroids to a single mapper (master mapper), where the average operation applies. It then broadcasts the averaged centroids data to every other mapper. </p>
<pre class="hljs"><code>comm_broadcastreduce(cenTable, pres);</code></pre><p>In push-pull, it first pushes centroids data from <em>cenTable</em> of local mapper to a <em>globalTable</em>, which is distributed across all the mappers. It then applies the average operation on <em>globalTable</em> from each mapper, and finally, pull the results from <em>globalTable</em> to update the local <em>cenTable</em>.</p>
<pre class="hljs"><code>Table&lt;DoubleArray&gt; globalTable = <span class="hljs-keyword">new</span> Table&lt;DoubleArray&gt;(<span class="hljs-number">0</span>, <span class="hljs-keyword">new</span> DoubleArrPlus());
comm_push_pull(cenTable, globalTable, pres);</code></pre><p>After finishing each iteration, call the <em>printTable</em> to check the centroids result</p>
<pre class="hljs"><code><span class="hljs-comment">//for iteration i, check 10 first centroids, each </span>
<span class="hljs-comment">//centroid prints out first 10 dimension</span>
printTable(cenTable, <span class="hljs-number">10</span>, <span class="hljs-number">10</span>, i);</code></pre><h3 id="step-8:-release-memory-and-store-centroids"><a class="header-link" href="#step-8:-release-memory-and-store-centroids"></a>Step 8: Release memory and store centroids</h3>
<p>After all of the iterations, release the allocated memory at DAAL side and for harp table object.
The centroids as output are stored at HDFS </p>
<pre class="hljs"><code><span class="hljs-comment">// free memory and record time</span>
cenTable_daal.freeDataMemory();
trainingdata_daal.freeDataMemory();
<span class="hljs-comment">// Write out centroids</span>
<span class="hljs-keyword">if</span> (<span class="hljs-keyword">this</span>.isMaster()) {
    KMUtil.storeCentroids(<span class="hljs-keyword">this</span>.conf, <span class="hljs-keyword">this</span>.cenDir,
    cenTable, <span class="hljs-keyword">this</span>.cenVecSize, <span class="hljs-string">"output"</span>);
}
cenTable.release();</code></pre><h2 id="invoke-harp-daal-k-means-via-python-interface"><a class="header-link" href="#invoke-harp-daal-k-means-via-python-interface"></a>Invoke Harp-DAAL K-means via Python Interface</h2>
<p>Harp-DAAL currently provides Python API, which interfaces 
Harp-DAAL with other python written applications. By just adding several lines of python code, you 
are able to deploy the original python application on Hadoop Cluster and boost the performance by 
leveraging DAAL kernels. </p>
<p>The python codes for image clustering is located at the path</p>
<pre class="hljs"><code><span class="hljs-variable">${PYTHONPATH}</span>/examples/scdemo/tutorial</code></pre><h3 id="step.1-run-image-clustering-on-15-scenery-dataset-with-python-scikit-learn-k-means"><a class="header-link" href="#step.1-run-image-clustering-on-15-scenery-dataset-with-python-scikit-learn-k-means"></a>Step.1 Run image clustering on 15 scenery dataset with Python Scikit-Learn K-means</h3>
<p>Run the pipeline from feature extraction, training, evaluation and finally check the clusters results.</p>
<pre class="hljs"><code>cd ${PYTHONPATH}/examples/scdemo/test
../tutorial/run_kmeans.sh</code></pre><p class="img-container"><img src="https://raw.githubusercontent.com/DSC-SPIDAL/harp/master/harp-daal-python/examples/scdemo/tutorial/imgcluster_runlocal.png" alt="screen shot of results"></p>
<h3 id="step.2-modify-to-invokes-harp-daal"><a class="header-link" href="#step.2-modify-to-invokes-harp-daal"></a>Step.2 Modify to invokes Harp-DAAL</h3>
<p><em>demo_kmeans_local.py</em> is the original python codes of image clustering without Harp-DAAL. </p>
<pre class="hljs"><code><span class="hljs-comment"># ############################################################################</span>
<span class="hljs-comment"># call kmeans module </span>
<span class="hljs-comment"># ############################################################################</span>
KMeans(init=<span class="hljs-string">'random'</span>, n_clusters=n_digits, max_iter=<span class="hljs-number">1000</span>, tol=<span class="hljs-number">0</span>, n_init=<span class="hljs-number">1</span>, n_jobs=<span class="hljs-number">1</span>)</code></pre><p><em>demo_kmeans_daal.py</em> replaces the above K-means module by a Harp-DAAL invocation</p>
<pre class="hljs"><code><span class="hljs-comment"># ############################################################################</span>
<span class="hljs-comment"># call Harp-DAAL Kmeans module </span>
<span class="hljs-comment"># ############################################################################</span>
DAALKMeans(n_clusters=n_digits, max_iter=<span class="hljs-number">1000</span>, n_init=<span class="hljs-number">1</span>, workdir=<span class="hljs-string">"/15scene-work"</span>)</code></pre><p>View all the modifications by</p>
<pre class="hljs"><code>diff ../tutorial/demo_kmeans_local.py ../tutorial/demo_kmeans_daal.py</code></pre><p class="img-container"><img src="https://raw.githubusercontent.com/DSC-SPIDAL/harp/master/harp-daal-python/examples/scdemo/tutorial/diffcode.png" alt="code diff"></p>
<h3 id="step.3-invoke-harp-daal"><a class="header-link" href="#step.3-invoke-harp-daal"></a>Step.3 Invoke Harp-DAAL</h3>
<pre class="hljs"><code>../tutorial/run_kmeans.sh daal</code></pre><h3 id="step.4-check-the-results-of-clustering"><a class="header-link" href="#step.4-check-the-results-of-clustering"></a>Step.4 Check the results of clustering</h3>
<p>Download the result files.</p>
<pre class="hljs"><code><span class="hljs-comment"># Copy a file from the running container to the host server</span>
sudo docker cp &lt;container-id&gt;:/harp/harp-daal-python/examples/scdemo/<span class="hljs-built_in">test</span>/local.pdf local.pdf

<span class="hljs-comment"># Copy a file from the server to local client</span>
scp &lt;username&gt;@&lt;server-ip&gt;:&lt;path-to&gt;/local.pdf local.pdf</code></pre><h3 id="step.5-(optional)-tune-harp-daal-kmeans-parameters"><a class="header-link" href="#step.5-(optional)-tune-harp-daal-kmeans-parameters"></a>Step.5 (Optional) Tune Harp-DAAL-Kmeans parameters</h3>
<p><em>daal_kmeans.py</em> contains the python API to Harp-DAAL-Kmeans Java codes.
In the <em><strong>init</strong></em> function, tune the arguments (parameters) and compare the performance.</p>
<pre class="hljs"><code>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, n_clusters=<span class="hljs-number">10</span>, max_iter=<span class="hljs-number">10</span>, init = <span class="hljs-string">'random'</span>, n_init = <span class="hljs-number">1</span>,
            n_node = <span class="hljs-number">1</span>, n_thread = <span class="hljs-number">8</span>, n_mem = <span class="hljs-number">10240</span>, workdir = <span class="hljs-string">"/kmeans-work"</span>
            )</span>:</span>
        <span class="hljs-string">"""
        n_clusters  ; set number of clusters
        max_iter    ; set maximum iteration number
        n_node      ; set mapper number
        n_thread    ; set thread number in each mapper
        init        ; set the centroid initialization method, 'random' by default
        n_init      ; set the number of runs to select the best model, 1 by default
        """</span></code></pre><ul class="list">
<li>Increase iteration number <em>max_iter</em> to check the changes in results</li>
<li>Increase the thread number <em>n_thread</em> to check the performance boost by multi-threading</li>
<li>Increase the mapper number <em>n_node</em> to check benefits from node-level parallelism.</li>
</ul>
    </article>
  </body>
</html>
